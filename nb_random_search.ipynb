{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random Search for Hyperparameter Optimization \n",
    "\n",
    "There are lots of different search strategies that you can use for hyperparmeter-optimization. Bergstra and Bengio showed that random search is an effective method. \n",
    "\n",
    "As with any hyperparameter search method, you will compare models based on validation set performance, then choose a final architecture from that, and finally measure your test set performance. \n",
    "\n",
    "\n",
    "## K-fold Validation & Hyperparameter Search\n",
    "\n",
    "This may be stating the obvious, but I found it useful for myself to understand how to integrate the hyperparameter search with k-fold validation. The pseudocode below is how I do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Search_loop= 1 to num_searches(\n",
    "\n",
    "\thyperparameters= random_selection \n",
    "\tvalidation_performance= 0\n",
    "\t\n",
    "\tloop_fold= 1 to K(\n",
    "\tvalidation_performance += Model(fold, hyperparameters)\n",
    "\t)\n",
    "\t\n",
    "\tmodel_average= validation_performance/K\n",
    "\tsave-to-csv(model_average, hyperparameters)\n",
    "\n",
    "Select hyperparameters that gave best model average\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When deciding on your number of searches (num_searches), here are a few things to consider:\n",
    "\n",
    "1. How much time you have\n",
    "2. How many other models you have\n",
    "3. How much things are improving over the runs\n",
    "\n",
    "You can leave things running over night and then decide whether to continue, the next morning.\n",
    "\n",
    "Here is the code I use for my hyperparameter search. Essentially, I'm embedding num_searches number of dictionaries into one big dictionary. Each dictionary contains a unique set of hyperparameters. \n",
    "\n",
    "I'm using numpy to sample numbers randomly from logspace, there are others ways to randomly sample number, but this method is especially simple. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparam_search(num_searches):\n",
    "    hyperparam_dict={}\n",
    "\n",
    "    for j in range(num_searches):\n",
    "        ## Randomely select hyperparameters\n",
    "        dict_name= 'parameter_set_%i' %j\n",
    "        hyperparam_dict[dict_name]={}\n",
    "\n",
    "        #Hidden Units\n",
    "        num_hidden_units= np.random.choice(np.logspace(start=2.1, stop=3, num=num_searches))\n",
    "        #chooses between 128 to 1000 hidden units, uniformly distributed along log space\n",
    "        rounded_hidden_units= np.round(num_hidden_units)\n",
    "        hyperparam_dict[dict_name]['hidden_units']= rounded_hidden_units\n",
    "\n",
    "        #Number of Layers\n",
    "        num_layers= np.random.uniform(low=1, high=3)\n",
    "        layer= np.round(num_layers)\n",
    "        hyperparam_dict[dict_name]['num_layer']=layer\n",
    "\n",
    "        #Learning Rate\n",
    "        learning_rate= np.random.choice(np.logspace(start=-5, stop=0, num=num_searches))\n",
    "        #Choose between 0.00001 and 1\n",
    "        rounded_learning_rate = np.around(learning_rate, decimals=3)\n",
    "        hyperparam_dict[dict_name]['learning_rate']= rounded_learning_rate\n",
    "        \n",
    "        #Batch Size\n",
    "        batch_size= np.random.randint(low=64, high=200)\n",
    "        hyperparam_dict[dict_name]['batch_size']= batch_size\n",
    "\n",
    "    return hyperparam_dict\n",
    "\n",
    "dict= hyperparam_search(num_searches)\n",
    "print dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When you print out the embedded dictionary, it gives you all the sets of parameters. You can store this dictionary and then call it when you're doing K-fold validation for your model. \n",
    "\n",
    "My print out looked like this, note that it sorted the dictionary randomly: \n",
    "\n",
    "{'parameter_set_2': {'learning_rate': 1.0000000000000001e-05, 'num_layer': 1.0, 'hidden_units': 355.0, 'batch_size': 169}, \n",
    "<br>'parameter_set_3': {'learning_rate': 1.0, 'num_layer': 2.0, 'hidden_units': 126.0, 'batch_size': 185}, \n",
    "<br>'parameter_set_0': {'learning_rate': 1.0, 'num_layer': 2.0, 'hidden_units': 1000.0, 'batch_size': 131}, \n",
    "<br>'parameter_set_1': {'learning_rate': 00.00316, 'num_layer': 1.0, 'hidden_units': 1000.0, 'batch_size': 135}, \n",
    "<br>'parameter_set_4': {'learning_rate': 0.00017782794100389227, 'num_layer': 1.0, 'hidden_units': 596.0, 'batch_size': 69}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}